
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from mcp_server.client import MCPSearchClient
from devmate.rag.retriever import LocalRAGRetriever

class ProgramAgent():

    def __init__(self, config):
        self.config = config
        #  == æç¤ºè¯ =================
        self.program_prompts = config.prompts[0]
        self.summary_prompts = config.prompts[1]
        self.knowledge_prompts = config.prompts[2]
        self.unify_context_prompts = config.prompts[3]
        #  ==========================
        
        #  == æ¨¡å‹åŸºç¡€é…ç½® =================
        llm_model = config.model_names[0]
        embeed_model = config.model_names[1]
        #  ==========================
        
        #  == ç§˜é’¥åŸºç¡€é…ç½® =================
        llm_model_key = config.api_keys[0]
        embeed_model_key = config.api_keys[1]
        
        # åˆå§‹åŒ–åç«¯å¤§æ¨¡å‹
        self.client = ChatOpenAI(
            model = llm_model,
            api_key = llm_model_key,
            base_url = config.api_base_url,
            temperature = 0.8,
            streaming = True
        )

        # åˆå§‹åŒ–MCPå®¢æˆ·ç«¯
        self.mcp_client = MCPSearchClient()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.rag_retriever = LocalRAGRetriever(embeed_model, embeed_model_key)


    def reset(self):
        """
        é‡ç½®å¯¹è¯çŠ¶æ€ï¼Œå°†æ‰€æœ‰å±æ€§æ¢å¤åˆ°åˆå§‹çŠ¶æ€
        """
        pass
    
    async def stop_chat(self):
        pass
    
    async def test_search(self):
        result = await self.mcp_client.search("åŒ—äº¬ä»Šå¤©çš„å¤©æ°”")
        return result
    
    def need_search(self, user_input: str) -> bool:
        """
        ç®€å•è§„åˆ™åˆ¤æ–­æ˜¯å¦éœ€è¦å¤–éƒ¨æœç´¢
        """
        keywords = ["å¤©æ°”", "ä»Šå¤©", "æœ€æ–°", "é™„è¿‘", "ç°åœ¨", "è·¯çº¿", "æ¨è"]
        return any(k in user_input for k in keywords)

    async def summarize_search_results(self, search_results: str) -> str:
        """
        Use LLM to summarize raw search results into concise context.
        """
        messages = [
            SystemMessage(content=self.summary_prompts.format(
                search_results=search_results
            ))
        ]

        summary_chunks = []
        async for chunk in self.client.astream(messages):
            if chunk.content:
                summary_chunks.append(chunk.content)

        return "".join(summary_chunks)

    async def unify_context(
        self,
        user_input: str,
        rag_chunks: list[dict],
        web_summary: str | None,
    ) -> str:
        rag_text = "\n".join(
            f"- ({c['source']}#chunk{c['chunk_id']}): {c['content']}"
            for c in rag_chunks
        )

        messages = [
            SystemMessage(
                content=self.unify_context_prompts.format(
                    question=user_input,
                    rag_context=rag_text,
                    web_context=web_summary or "None",
                )
            )
        ]

        chunks = []
        async for chunk in self.client.astream(messages):
            if chunk.content:
                chunks.append(chunk.content)

        return "".join(chunks)

    async def stream(self, user_input: str, user_id: str, conversation_id: str = ""):
        messages = [
            SystemMessage(content=self.program_prompts),
            HumanMessage(content=user_input),
        ]

        # ====== 1ï¸âƒ£ RAG æœ¬åœ°æ£€ç´¢ ======
        yield "[RAG] Retrieving local knowledge...\n"
        rag_chunks = self.rag_retriever.retrieve(user_input)

        # ====== 2ï¸âƒ£ MCP Web Searchï¼ˆå¯é€‰ï¼‰ ======
        web_summary = None
        if self.need_search(user_input):
            yield "[Tool] Searching web...\n"
            raw_search = await self.mcp_client.search(user_input)
            web_summary = await self.summarize_search_results(raw_search)

        # ====== 3ï¸âƒ£ ç»Ÿä¸€ä¸Šä¸‹æ–‡æ‘˜è¦ï¼ˆğŸ”¥ Gï¼‰ ======
        yield "[Thinking] Consolidating knowledge from multiple sources...\n"
        unified_context = await self.unify_context(
            user_input,
            rag_chunks,
            web_summary
        )

        # ====== 4ï¸âƒ£ æ³¨å…¥ä¸Šä¸‹æ–‡ï¼ˆğŸ”¥ Hï¼‰ ======
        # ä¿è¯æœ‰æ•°æ®è¾“å…¥
        if len(unified_context):
            messages.append(
                SystemMessage(
                    content=(self.knowledge_prompts.format(unified_context = unified_context))
                )
            )

        # ====== 5ï¸âƒ£ æœ€ç»ˆå›ç­”ï¼ˆStreamingï¼‰ ======
        for chunk in self.client.stream(messages):
            if chunk.content:
                print(chunk.content)
                yield chunk.content
